{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "import copy\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "from config import gen_args\n",
    "from data_utils import load_data, get_scene_info\n",
    "from models import Model\n",
    "from utils import create_instance_colors, set_seed,  Tee, count_parameters\n",
    "from data_utils import get_env_group, prepare_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Planner(object):\n",
    "\n",
    "    def __init__(self, args, n_his, n_particle, n_shape, scene_params, model, dist_func, use_gpu, beta_filter=1, env=\"gripper\"):\n",
    "        self.args = args\n",
    "        self.n_his = n_his\n",
    "        self.n_particle = n_particle\n",
    "        self.n_shape = n_shape\n",
    "        self.scene_params = scene_params\n",
    "        self.beta_filter = beta_filter\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.dist_func = dist_func\n",
    "        self.use_gpu = use_gpu\n",
    "        self.n_sample = 100\n",
    "        self.reward_weight = 1\n",
    "    \n",
    "    def trajectory_optimization(\n",
    "        self,\n",
    "        state_cur,      # [n_his, state_dim]\n",
    "        state_goal,     # [state_dim]\n",
    "        act_seq,        # [n_his + n_look_ahead - 1, action_dim]\n",
    "        n_sample,\n",
    "        n_look_ahead,\n",
    "        n_update_iter,\n",
    "        action_lower_lim,\n",
    "        action_upper_lim,\n",
    "        action_lower_delta_lim,\n",
    "        action_upper_delta_lim,\n",
    "        use_gpu,\n",
    "        reward_scale_factor=1.):\n",
    "\n",
    "        for i in range(n_update_iter):\n",
    "            act_seqs = self.sample_action_sequences(\n",
    "                act_seq, n_sample,\n",
    "                action_lower_lim, action_upper_lim,\n",
    "                action_lower_delta_lim, action_upper_delta_lim)\n",
    "            state_seqs = self.model_rollout(\n",
    "                state_cur, act_seqs, n_look_ahead, use_gpu)\n",
    "            reward_seqs = reward_scale_factor * self.evaluate_traj(state_seqs, state_goal)\n",
    "            reward_seqs = reward_seqs.data.cpu().numpy()\n",
    "\n",
    "            print('update_iter %d/%d, max: %.4f, mean: %.4f, std: %.4f' % (\n",
    "                i, n_update_iter, np.max(reward_seqs), np.mean(reward_seqs), np.std(reward_seqs)))\n",
    "\n",
    "            act_seq = self.optimize_action(act_seqs, reward_seqs)\n",
    "\n",
    "        # act_seq: [n_his + n_look_ahead - 1, action_dim]\n",
    "        return act_seq\n",
    "\n",
    "    def sample_action_sequences(\n",
    "        self,\n",
    "        init_act_seq,   # [n_his + n_look_ahead - 1, action_dim]\n",
    "        n_sample,       # number of action tarjs to sample\n",
    "        action_lower_lim,\n",
    "        action_upper_lim,\n",
    "        action_lower_delta_lim,\n",
    "        action_upper_delta_lim,\n",
    "        noise_type='normal'):\n",
    "        action_dim = init_act_seq.shape[-1]\n",
    "        beta_filter = self.beta_filter\n",
    "\n",
    "        # act_seqs: [n_sample, N, action_dim]\n",
    "        # act_seqs_delta: [n_sample, N - 1, action_dim]\n",
    "        act_seqs = np.stack([init_act_seq] * n_sample)\n",
    "        act_seqs_delta = np.stack([init_act_seq[1:] - init_act_seq[:-1]] * n_sample)\n",
    "\n",
    "        # [n_sample, action_dim]\n",
    "        act_residual = np.zeros([n_sample, action_dim])\n",
    "\n",
    "        # only add noise to future actions\n",
    "        # init_act_seq[:(n_his - 1)] are past actions\n",
    "        # The action we are optimizing for the current timestep is act_seq[n_his - 1]\n",
    "\n",
    "        # actions that go as input to the dynamics network\n",
    "        for i in range(self.n_his - 2, init_act_seq.shape[0] - 1):\n",
    "\n",
    "            if noise_type == \"normal\":\n",
    "\n",
    "                if self.env in ['FluidManipClip', 'FluidManipClip_wKuka_wColor']:\n",
    "                    # [n_sample, action_dim]\n",
    "                    sigma_pos = 0.002\n",
    "                    noise_pos = np.random.normal(0, sigma_pos, (n_sample, 3))    # position\n",
    "\n",
    "                    sigma_angle = 0.01\n",
    "                    noise_angle = np.random.normal(0, sigma_angle, (n_sample, 1))  # angle\n",
    "\n",
    "                    noise_sample = np.concatenate([noise_pos, noise_angle], -1)\n",
    "\n",
    "                elif self.env in ['FluidShakeWithIce_1000', 'FluidShakeWithIce_wKuka_wColor_wGripper']:\n",
    "                    # [n_sample, action_dim]\n",
    "                    sigma_pos = 0.005\n",
    "                    noise_sample = np.random.normal(0, sigma_pos, (n_sample, 2))\n",
    "                else:\n",
    "                    sigma = 0.01\n",
    "                    noise_sample = np.random.normal(0, sigma, (n_sample, 3))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"unknown noise type: %s\" % (noise_type))\n",
    "\n",
    "            act_residual = beta_filter * noise_sample + act_residual * (1. - beta_filter)\n",
    "            act_seqs_delta[:, i] += act_residual\n",
    "\n",
    "\n",
    "            # clip delta lim\n",
    "            act_seqs_delta[:, i] = np.clip(\n",
    "                act_seqs_delta[:, i], action_lower_delta_lim, action_upper_delta_lim)\n",
    "\n",
    "            act_seqs[:, i + 1] = act_seqs[:, i] + act_seqs_delta[:, i]\n",
    "\n",
    "            # clip absolute lim\n",
    "            act_seqs[:, i + 1] = np.clip(\n",
    "                act_seqs[:, i + 1], action_lower_lim, action_upper_lim)\n",
    "\n",
    "\n",
    "        # print(act_seqs[:5])\n",
    "        # time.sleep(100)\n",
    "\n",
    "        '''\n",
    "        print(init_act_seq[:, 3])\n",
    "        print(np.mean(act_seqs[:, :, 3], 0))\n",
    "        time.sleep(10)\n",
    "        '''\n",
    "\n",
    "        # act_seqs: [n_sample, -1, action_dim]\n",
    "        return act_seqs\n",
    "\n",
    "    def expand(self, info, n_sample):\n",
    "        length = len(info.shape)\n",
    "        if length == 2:\n",
    "            info = info.expand([n_sample, -1])\n",
    "        elif length == 3:\n",
    "            info = info.expand([n_sample, -1, -1])\n",
    "        elif length == 4:\n",
    "            info = info.expand([n_sample, -1, -1, -1])\n",
    "        return info\n",
    "\n",
    "    def state_action(self, state_cur, act_cur):\n",
    "        state = state_cur[1]  # n_sample x n_his x (n_particle + n_shape) x state_dim\n",
    "        if self.env == \"gripper\":\n",
    "            shapes = state[:, :, self.n_particle+1:, :] ### TODO: This is for gripper\n",
    "        else:\n",
    "             raise NotImplementedError\n",
    "        shapes_diff = shapes[:, 1:, :, :] - shapes[:, :-1, :, :]\n",
    "        shapes_diff_act = act_cur * 0.02\n",
    "        pdb.set_trace()\n",
    "\n",
    "\n",
    "    def prepare_rollout(self):\n",
    "        B = self.n_sample\n",
    "        self.scene_params = self.scene_params.expand(self.n_sample, -1)\n",
    "        self.group_gt = get_env_group(self.args, self.n_particle, self.scene_params, use_gpu=self.use_gpu)\n",
    "        self.memory_init = self.model.init_memory(B, n_particle + n_shape)\n",
    "\n",
    "    def expand_inputs(self, inputs):\n",
    "        inputs_new = []\n",
    "        for infos in inputs:\n",
    "            if infos is not None:\n",
    "                if isinstance(infos, list):\n",
    "                    my_info = []\n",
    "                    for info in infos:\n",
    "                        info = self.expand(info, n_sample)\n",
    "                        my_info.append(info)\n",
    "                    infos = my_info\n",
    "                else:\n",
    "                    infos = self.expand(infos, n_sample)\n",
    "            inputs_new.append(infos)\n",
    "        return inputs_new\n",
    "\n",
    "    def model_rollout(\n",
    "        self,\n",
    "        state_cur,      # [1, n_his, state_dim]\n",
    "        act_seqs_np,    # [n_sample, -1, action_dim]\n",
    "        n_look_ahead,\n",
    "        use_gpu):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        act_seqs = torch.FloatTensor(act_seqs_np).float()\n",
    "        if use_gpu:\n",
    "            act_seqs = act_seqs.cuda()\n",
    "        n_sample = act_seqs.shape[0]\n",
    "\n",
    "        # states_cur: [n_sample, n_his, state_dim]\n",
    "\n",
    "\n",
    "        states_pred_list = []\n",
    "        assert n_look_ahead == act_seqs.shape[1] - n_his + 1\n",
    "        if self.use_gpu:\n",
    "            state_cur = self.expand(state_cur.unsqueeze(0), n_sample).cuda()\n",
    "        else:\n",
    "            state_cur = self.expand(state_cur.unsqueeze(0), n_sample)\n",
    "        for i in range(min(n_look_ahead, act_seqs.shape[1] - n_his + 1)):\n",
    "            # state_cur = torch.tensor(state_cur_np, device=device).float()\n",
    "            print(f\"{i}/{min(n_look_ahead, act_seqs.shape[1] - n_his + 1)}\")\n",
    "            attrs = []\n",
    "            Rr_curs = []\n",
    "            Rs_curs = []\n",
    "            max_n_rel = 0\n",
    "            for j in range(n_sample):\n",
    "                # pdb.set_trace()\n",
    "                attr, _, Rr_cur, Rs_cur, cluster_onehot = prepare_input(state_cur[j][-1].cpu().numpy(), self.n_particle,\n",
    "                                                                    self.n_shape, self.args, stdreg=self.args.stdreg)\n",
    "                if use_gpu:\n",
    "                    attr = attr.cuda()\n",
    "                    Rr_cur = Rr_cur.cuda()\n",
    "                    Rs_cur = Rs_cur.cuda()\n",
    "                max_n_rel = max(max_n_rel, Rr_cur.size(0))\n",
    "                attr = attr.unsqueeze(0)\n",
    "                Rr_cur = Rr_cur.unsqueeze(0)\n",
    "                Rs_cur = Rs_cur.unsqueeze(0)\n",
    "                # state_cur = state_cur.unsqueeze(0)\n",
    "                attrs.append(attr)\n",
    "                Rr_curs.append(Rr_cur)\n",
    "                Rs_curs.append(Rs_cur)\n",
    "\n",
    "            attrs = torch.cat(attrs, dim=0)\n",
    "            for k in range(len(Rr_curs)):\n",
    "                Rr, Rs = Rr_curs[k], Rs_curs[k]\n",
    "                if self.use_gpu:\n",
    "                    Rr = torch.cat([Rr, torch.zeros((1, max_n_rel - Rr.size(1), self.n_particle + self.n_shape), device='cuda')], 1)\n",
    "                    Rs = torch.cat([Rs, torch.zeros((1, max_n_rel - Rs.size(1), self.n_particle + self.n_shape), device='cuda')], 1)\n",
    "                else:\n",
    "                    Rr = torch.cat([Rr, torch.zeros(1, max_n_rel - Rr.size(1), self.n_particle + self.n_shape)], 1)\n",
    "                    Rs = torch.cat([Rs, torch.zeros(1, max_n_rel - Rs.size(1), self.n_particle + self.n_shape)], 1)\n",
    "                Rr_curs[k], Rs_curs[k] = Rr, Rs\n",
    "\n",
    "            Rr_curs = torch.cat(Rr_curs, dim=0)\n",
    "            Rs_curs = torch.cat(Rs_curs, dim=0)\n",
    "\n",
    "            inputs = [attrs, state_cur, Rr_curs, Rs_curs, self.memory_init, self.group_gt, None]\n",
    "            # inputs = self.expand_inputs(inputs)\n",
    "\n",
    "\n",
    "            act_cur = act_seqs[:, i:i+n_his]\n",
    "            # state_cur_act = self.state_action(state_new, act_cur)\n",
    "            # states_pred: [n_sample, state_dim]\n",
    "            pred_pos, pred_motion_norm, std_cluster  = self.model.predict_dynamics(inputs)\n",
    "            new_shape1 = inputs[1][:, -1, self.n_particle+1, :] + act_cur[:, -1, :] * 0.02\n",
    "            if self.use_gpu:\n",
    "                new_shape2 = inputs[1][:, -1, self.n_particle+2, :] + act_cur[:, -1, :] * 0.02 * torch.cuda.FloatTensor([-1, 1, 1])\n",
    "            else:\n",
    "                new_shape2 = inputs[1][:, -1, self.n_particle+2, :] + act_cur[:, -1, :] * 0.02 * torch.FloatTensor([-1, 1, 1])\n",
    "\n",
    "            pred_pos = torch.cat([pred_pos, state_cur[:, -1, -3, :].unsqueeze(1), new_shape1.unsqueeze(1), new_shape2.unsqueeze(1)], 1)\n",
    "\n",
    "            state_cur = torch.cat([state_cur[:, 1:], pred_pos.unsqueeze(1)], 1)\n",
    "            states_pred_list.append(pred_pos[:, :self.n_particle, :])\n",
    "        # states_pred_tensor: [n_sample, n_look_ahead, state_dim]\n",
    "        states_pred_tensor = torch.stack(states_pred_list, dim=1)\n",
    "\n",
    "        return states_pred_tensor #.data.cpu().numpy()\n",
    "\n",
    "    def evaluate_traj(\n",
    "        self,\n",
    "        state_seqs,     # [n_sample, n_look_ahead, state_dim]\n",
    "        state_goal,     # [state_dim]\n",
    "    ):\n",
    "        # reward_seqs = -np.mean(np.sum((state_seqs[:, -1] - state_goal)**2, 2), 1)\n",
    "        goal = state_goal.expand(self.n_sample, -1, -1)\n",
    "        reward_seqs = emd(state_seqs[:, -1], goal)\n",
    "        # reward_seqs: [n_sample]\n",
    "        return reward_seqs\n",
    "\n",
    "    def optimize_action_CEM(    # Cross Entropy Method (CEM)\n",
    "        self,\n",
    "        act_seqs,       # [n_sample, -1, action_dim]\n",
    "        reward_seqs     # [n_sample]\n",
    "    ):\n",
    "\n",
    "        idx = np.argsort(reward_seqs)\n",
    "        # [-1, action_dim]\n",
    "        return np.mean(act_seqs[idx[-5:], :, :], 0)\n",
    "\n",
    "    def optimize_action(   # Model-Predictive Path Integral (MPPI)\n",
    "        self,\n",
    "        act_seqs,       # [n_sample, -1, action_dim]\n",
    "        reward_seqs     # [n_sample]\n",
    "    ):\n",
    "        # reward_base = self.args.reward_base\n",
    "        reward_base = np.mean(reward_seqs)\n",
    "        reward_weight = self.reward_weight\n",
    "\n",
    "        # [n_sample, 1, 1]\n",
    "        reward_seqs_exp = np.exp(reward_weight * (reward_seqs - reward_base))\n",
    "        reward_seqs_exp = reward_seqs_exp.reshape(-1, 1, 1)\n",
    "\n",
    "        # [-1, action_dim]\n",
    "        eps = 1e-8\n",
    "        act_seq = (reward_seqs_exp * act_seqs).sum(0) / (np.sum(reward_seqs_exp) + eps)\n",
    "\n",
    "        # [-1, action_dim]\n",
    "        return act_seq\n",
    "\n",
    "\n",
    "class EarthMoverLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EarthMoverLoss, self).__init__()\n",
    "\n",
    "    def em_distance(self, x, y):\n",
    "        # x: [B, N, D]\n",
    "        # y: [B, M, D]\n",
    "        x_ = x[:, :, None, :].repeat(1, 1, y.size(1), 1)  # x: [B, N, M, D]\n",
    "        y_ = y[:, None, :, :].repeat(1, x.size(1), 1, 1)  # y: [B, N, M, D]\n",
    "        dis = torch.norm(torch.add(x_, -y_), 2, dim=3)  # dis: [B, N, M]\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        # x.requires_grad = True\n",
    "        # y.requires_grad = True\n",
    "        for i in range(dis.shape[0]):\n",
    "            cost_matrix = dis[i].detach().cpu().numpy()\n",
    "            try:\n",
    "                ind1, ind2 = scipy.optimize.linear_sum_assignment(cost_matrix, maximize=False)\n",
    "            except:\n",
    "                import pdb; pdb.set_trace()\n",
    "            x_list.append(x[i, ind1])\n",
    "            y_list.append(y[i, ind2])\n",
    "            # x[i] = x[i, ind1]\n",
    "            # y[i] = y[i, ind2]\n",
    "        new_x = torch.stack(x_list)\n",
    "        new_y = torch.stack(y_list)\n",
    "        emd = torch.mean(torch.norm(torch.add(new_x, -new_y), 2, dim=2), dim=1)\n",
    "        return emd\n",
    "\n",
    "    def __call__(self, pred, label):\n",
    "        # pred: [B, N, D]\n",
    "        # label: [B, M, D]\n",
    "        return self.em_distance(pred, label)\n",
    "\n",
    "\n",
    "\n",
    "def set_action_limit(all_actions, ctrl_init_idx):\n",
    "    action_lower_lim = np.min(np.array(all_actions[ctrl_init_idx - 1:]), 0)\n",
    "    action_upper_lim = np.max(np.array(all_actions[ctrl_init_idx - 1:]), 0)\n",
    "    action_lim_range = action_upper_lim - action_lower_lim\n",
    "    action_lower_lim -= action_lim_range * 0.1\n",
    "    action_upper_lim += action_lim_range * 0.1\n",
    "    print('action_lower_lim', action_lower_lim)\n",
    "    print('action_upper_lim', action_upper_lim)\n",
    "    action_lower_delta_lim = np.min(np.array(all_actions[ctrl_init_idx:]) - np.array(all_actions[ctrl_init_idx - 1:-1]),\n",
    "                                    0)\n",
    "    action_upper_delta_lim = np.max(np.array(all_actions[ctrl_init_idx:]) - np.array(all_actions[ctrl_init_idx - 1:-1]),\n",
    "                                    0)\n",
    "    action_delta_lim_range = action_upper_delta_lim - action_lower_delta_lim\n",
    "    action_lower_delta_lim -= action_delta_lim_range * 0.1\n",
    "    action_upper_delta_lim += action_delta_lim_range * 0.1\n",
    "    print('action_lower_delta_lim', action_lower_delta_lim)\n",
    "    print('action_upper_delta_lim', action_upper_delta_lim)\n",
    "\n",
    "    return action_lower_lim, action_upper_lim, action_lower_delta_lim, action_upper_delta_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['foo', '--env', 'Gripper', \n",
    "            '--stage', 'dy', \n",
    "            '--eval_epoch', '92', \n",
    "            '--eval_iter', '472', \n",
    "            '--eval_set', 'train', \n",
    "            '--verbose_data', '0',\n",
    "            '--n_his', '4', \n",
    "            '--augment', '0.05']\n",
    "\n",
    "args = gen_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plb.optimizer.optim import Adam\n",
    "from plb.engine.taichi_env import TaichiEnv\n",
    "from plb.config.default_config import get_cfg_defaults, CN\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import taichi as ti\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yacs.config import CfgNode\n",
    "from plb.config import load\n",
    "cfg = load(\"/viscam/u/hshi74/projects/deformable/PlasticineLab/plb/envs/gripper.yml\")\n",
    "print(cfg)\n",
    "env = TaichiEnv(cfg, nn=False, loss=False)\n",
    "env.initialize()\n",
    "state = env.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_state(**state)\n",
    "taichi_env = env\n",
    "action_dim = taichi_env.primitives.action_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_parameters(env: TaichiEnv, yield_stress, E, nu):\n",
    "    env.simulator.yield_stress.fill(yield_stress)\n",
    "    _mu, _lam = E / (2 * (1 + nu)), E * nu / ((1 + nu) * (1 - 2 * nu))  # Lame parameters\n",
    "    env.simulator.mu.fill(_mu)\n",
    "    env.simulator.lam.fill(_lam)\n",
    "    \n",
    "import glob\n",
    "i = 0\n",
    "task_name = 'gripper'\n",
    "    \n",
    "env.set_state(**state)\n",
    "taichi_env = env\n",
    "env.renderer.camera_pos[0] = 0.5#np.array([float(i) for i in (0.5, 2.5, 0.5)]) #(0.5, 2.5, 0.5)  #.from_numpy(np.array([[0.5, 2.5, 0.5]]))\n",
    "env.renderer.camera_pos[1] = 2.5\n",
    "env.renderer.camera_pos[2] = 0.5\n",
    "env.renderer.camera_rot = (1.57, 0.0)\n",
    "\n",
    "env.primitives.primitives[0].set_state(0, [0.3, 0.4, 0.5, 1, 0, 0, 0])\n",
    "env.primitives.primitives[1].set_state(0, [0.7, 0.4, 0.5, 1, 0, 0, 0])\n",
    "set_parameters(env, yield_stress=300, E=800, nu=0.2) # 200， 5e3, 0.2\n",
    "\n",
    "env.render('plt')\n",
    "\n",
    "action_dim = taichi_env.primitives.action_dim\n",
    "horizon = 20\n",
    "action = np.zeros([horizon, action_dim])\n",
    "count = i / 50\n",
    "# map count from 0...1 to -1...1\n",
    "count = 2 * count - 1\n",
    "updown = count * 0.12\n",
    "grip_motion = np.random.uniform(0.20, 0.24)\n",
    "action[:20, 2] = updown#0.1\n",
    "action[:20, 1] = -0.7\n",
    "action[:20, 8] = updown#0.1\n",
    "action[:20, 7] = -0.7\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "imgs = []\n",
    "\n",
    "\n",
    "for idx, act in enumerate(tqdm(action, total=horizon)):\n",
    "\n",
    "    obs = env.step(act)\n",
    "    if task_name == 'gripper':\n",
    "        primitive_state = [env.primitives.primitives[0].get_state(0), env.primitives.primitives[1].get_state(0)]\n",
    "    else:\n",
    "        primitive_state = [env.primitives.primitives[0].get_state(0)]\n",
    "\n",
    "    if (idx+1) % 5 == 0:\n",
    "        env.render(mode='plt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.random_seed)\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "\n",
    "# load dynamics model\n",
    "model = Model(args, use_gpu)\n",
    "print(\"model_kp #params: %d\" % count_parameters(model))\n",
    "model_name = 'net_epoch_%d_iter_%d.pth' % (args.eval_epoch, args.eval_iter)\n",
    "model_dir = 'files_dy_27-Sep-2021-17:22:31.573683_nHis4_aug0.05_emd_uh_clip_seqlen7_uhw0.02_clipw0.0'\n",
    "model_path = os.path.join('dump/dump_Gripper/' + model_dir, model_name)\n",
    "if use_gpu:\n",
    "    pretrained_dict = torch.load(model_path)\n",
    "else:\n",
    "    pretrained_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model_dict = model.state_dict()\n",
    "# only load parameters in dynamics_predictor\n",
    "pretrained_dict = {\n",
    "    k: v for k, v in pretrained_dict.items() \\\n",
    "    if 'dynamics_predictor' in k and k in model_dict}\n",
    "model.load_state_dict(pretrained_dict, strict=False)\n",
    "model.eval()\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "# load data (state, actions)\n",
    "task_name = \"gripper\"\n",
    "rollout_dir = f\"./data/data_Gripper/train/\"\n",
    "n_vid = 1\n",
    "# n_frame = 49\n",
    "data_names = ['positions', 'shape_quats', 'scene_params']\n",
    "\n",
    "for i in range(n_vid):\n",
    "    B = 1\n",
    "    n_particle, n_shape = 0, 0\n",
    "    all_p = []\n",
    "    all_actions = []\n",
    "    all_s = []\n",
    "    for t in range(args.time_step):\n",
    "        if task_name == \"gripper\":\n",
    "            frame_path = os.path.join(rollout_dir, str(i).zfill(3), str(t) + '.h5')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        this_data = load_data(data_names, frame_path)\n",
    "        if n_particle == 0 and n_shape == 0:\n",
    "            n_particle, n_shape, scene_params = get_scene_info(this_data)\n",
    "            scene_params = torch.FloatTensor(scene_params).unsqueeze(0)\n",
    "\n",
    "        states = this_data[0]\n",
    "        if t >= 1:\n",
    "            action = (states[-2]-prev_states[-2]) / 0.02\n",
    "            all_p.append(states)\n",
    "            all_actions.append(action)\n",
    "            all_s.append(this_data[1])\n",
    "        prev_states = states\n",
    "\n",
    "\n",
    "ctrl_init_idx = 4\n",
    "n_look_ahead = 20\n",
    "n_update_delta = 1\n",
    "n_his = 4\n",
    "n_sample = 100\n",
    "n_update_iter_init = 1\n",
    "n_update_iter = 1\n",
    "emd = EarthMoverLoss()\n",
    "\n",
    "planner = Planner(args=args, n_his=n_his, n_particle=n_particle, n_shape=n_shape, scene_params=scene_params,\n",
    "                  model=model, dist_func=emd, use_gpu=use_gpu)\n",
    "planner.prepare_rollout()\n",
    "actions = all_actions[:n_his-1]\n",
    "# import pdb; pdb.set_trace()\n",
    "for i in range(n_look_ahead):\n",
    "    actions.append(actions[-1])\n",
    "actions = np.array(actions)\n",
    "action_lower_lim, action_upper_lim, action_lower_delta_lim, action_upper_delta_lim = \\\n",
    "    set_action_limit(all_actions=all_actions, ctrl_init_idx=ctrl_init_idx)\n",
    "\n",
    "\n",
    "st_idx = ctrl_init_idx\n",
    "ed_idx = ctrl_init_idx +1#+ n_look_ahead\n",
    "\n",
    "p_list = copy.copy(all_p[:n_his])\n",
    "s_list = copy.copy(all_s[:n_his])\n",
    "if use_gpu:\n",
    "    state_goal = torch.cuda.FloatTensor(all_p[-1]).unsqueeze(0)[:, :300, :]\n",
    "else:\n",
    "    state_goal = torch.FloatTensor(all_p[-1]).unsqueeze(0)[:, :300, :]\n",
    "### we note, we now have n_his states, n_his - 1 actions\n",
    "for i in range(st_idx, ed_idx):\n",
    "    print(\"\\n### Step %d/%d\" % (i, ed_idx))\n",
    "\n",
    "\n",
    "    \n",
    "    if i == st_idx or i % n_update_delta == 0:\n",
    "        # update the action sequence every n_update_delta iterations\n",
    "        with torch.set_grad_enabled(False):\n",
    "            state_cur = torch.FloatTensor(np.stack(p_list[-n_his:]))\n",
    "            # s_cur = torch.FloatTensor(np.stack(s_list[-n_his:]))\n",
    "\n",
    "\n",
    "            action = planner.trajectory_optimization(\n",
    "                                        state_cur=state_cur,\n",
    "                                        state_goal=state_goal,\n",
    "                                        act_seq=actions[i-n_his:],\n",
    "                                        n_sample=n_sample,\n",
    "                                        n_look_ahead=n_look_ahead - (i - ctrl_init_idx),\n",
    "                                        n_update_iter=n_update_iter_init if i == st_idx else n_update_iter,\n",
    "                                        action_lower_lim=action_lower_lim,\n",
    "                                        action_upper_lim=action_upper_lim,\n",
    "                                        action_lower_delta_lim=action_lower_delta_lim,\n",
    "                                        action_upper_delta_lim=action_upper_delta_lim,\n",
    "                                        use_gpu=use_gpu)\n",
    "            print(action.shape)\n",
    "#             obs = env.step(act)\n",
    "env_act = np.zeros([action.shape[0], 12])\n",
    "env_act[:, :3] = action\n",
    "env_act[:, 6:9] = action * np.array([-1, 1, 1])\n",
    "for i in range(action.shape[0]):\n",
    "    env.step(env_act[i])\n",
    "    env.render('plt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
