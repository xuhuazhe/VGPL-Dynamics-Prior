model_kp #params: 443560
Spinning at iteration 0
The first frame: [ 0.42896205 -0.00766296  0.07451145] +- [0.017 0.017 0.006]
sampling: max: -0.0274, mean: -0.0295, std: 0.0008
Selected top reward seqs: tensor([-0.0286, -0.0286, -0.0285, -0.0285, -0.0285, -0.0285, -0.0284, -0.0284,
        -0.0284, -0.0284, -0.0282, -0.0280, -0.0279, -0.0279, -0.0278, -0.0278,
        -0.0277, -0.0275, -0.0275, -0.0274])
Batch 0/20:
reward seqs after 18 iterations: tensor([-0.0279], grad_fn=<CatBackward>)
Batch 1/20:
reward seqs after 10 iterations: tensor([-0.0285], grad_fn=<CatBackward>)
Batch 2/20:
reward seqs after 8 iterations: tensor([-0.0274], grad_fn=<CatBackward>)
Batch 3/20:
reward seqs after 9 iterations: tensor([-0.0278], grad_fn=<CatBackward>)
Batch 4/20:
reward seqs after 6 iterations: tensor([-0.0281], grad_fn=<CatBackward>)
Batch 5/20:
reward seqs after 13 iterations: tensor([-0.0275], grad_fn=<CatBackward>)
Batch 6/20:
reward seqs after 9 iterations: tensor([-0.0277], grad_fn=<CatBackward>)
Batch 7/20:
reward seqs after 4 iterations: tensor([-0.0281], grad_fn=<CatBackward>)
Batch 8/20:
reward seqs after 13 iterations: tensor([-0.0272], grad_fn=<CatBackward>)
Batch 9/20:
reward seqs after 5 iterations: tensor([-0.0280], grad_fn=<CatBackward>)
Batch 10/20:
reward seqs after 9 iterations: tensor([-0.0273], grad_fn=<CatBackward>)
Batch 11/20:
reward seqs after 5 iterations: tensor([-0.0276], grad_fn=<CatBackward>)
Batch 12/20:
reward seqs after 13 iterations: tensor([-0.0270], grad_fn=<CatBackward>)
Batch 13/20:
reward seqs after 4 iterations: tensor([-0.0277], grad_fn=<CatBackward>)
Batch 14/20:
reward seqs after 3 iterations: tensor([-0.0272], grad_fn=<CatBackward>)
Batch 15/20:
reward seqs after 17 iterations: tensor([-0.0265], grad_fn=<CatBackward>)
Batch 16/20:
reward seqs after 9 iterations: tensor([-0.0273], grad_fn=<CatBackward>)
Batch 17/20:
reward seqs after 10 iterations: tensor([-0.0268], grad_fn=<CatBackward>)
Batch 18/20:
reward seqs after 11 iterations: tensor([-0.0271], grad_fn=<CatBackward>)
Batch 19/20:
reward seqs after 19 iterations: tensor([-0.0267], grad_fn=<CatBackward>)
Loss: 0.02651728130877018
Optimal set of params:
mid_point: tensor([[0.4879, 0.1400, 0.4963]])
angle: tensor([0.6763], grad_fn=<SelectBackward>)
gripper_rate: tensor([0.0113], grad_fn=<ClampBackward1>)
Optimal init pose seq: tensor([[0.1759, 0.1400, 0.7466, 1.0000, 0.0000, 0.0000, 0.0000]],
       grad_fn=<SliceBackward>)
=============== Iteration 0 -> model_loss: tensor([0.0265]) ===============
torch.Size([1, 11, 14]) torch.Size([1, 40, 12])
Best init pose: tensor([[0.1759, 0.1400, 0.7466, 1.0000, 0.0000, 0.0000, 0.0000]])
Best model loss: tensor([0.0265])
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 1
