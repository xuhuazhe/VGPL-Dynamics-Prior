model_kp #params: 443560
Spinning at iteration 0
The first frame: [ 0.42895059 -0.00614593  0.07408052] +- [0.017 0.017 0.006]
sampling: max: -0.0335, mean: -0.0377, std: 0.0015
Selected top reward seqs: tensor([-0.0359, -0.0358, -0.0357, -0.0356, -0.0356, -0.0356, -0.0356, -0.0351,
        -0.0350, -0.0348, -0.0346, -0.0345, -0.0345, -0.0344, -0.0343, -0.0338,
        -0.0338, -0.0337, -0.0336, -0.0335])
Batch 0/20:
reward seqs after 12 iterations: tensor([-0.0366], grad_fn=<CatBackward>)
Batch 1/20:
reward seqs after 20 iterations: tensor([-0.0335], grad_fn=<CatBackward>)
Batch 2/20:
reward seqs after 22 iterations: tensor([-0.0348], grad_fn=<CatBackward>)
Batch 3/20:
reward seqs after 5 iterations: tensor([-0.0351], grad_fn=<CatBackward>)
Batch 4/20:
reward seqs after 3 iterations: tensor([-0.0334], grad_fn=<CatBackward>)
Batch 5/20:
reward seqs after 17 iterations: tensor([-0.0345], grad_fn=<CatBackward>)
Batch 6/20:
reward seqs after 10 iterations: tensor([-0.0349], grad_fn=<CatBackward>)
Batch 7/20:
reward seqs after 18 iterations: tensor([-0.0339], grad_fn=<CatBackward>)
Batch 8/20:
reward seqs after 6 iterations: tensor([-0.0348], grad_fn=<CatBackward>)
Batch 9/20:
reward seqs after 4 iterations: tensor([-0.0347], grad_fn=<CatBackward>)
Batch 10/20:
reward seqs after 11 iterations: tensor([-0.0345], grad_fn=<CatBackward>)
Batch 11/20:
reward seqs after 3 iterations: tensor([-0.0350], grad_fn=<CatBackward>)
Batch 12/20:
reward seqs after 20 iterations: tensor([-0.0336], grad_fn=<CatBackward>)
Batch 13/20:
reward seqs after 4 iterations: tensor([-0.0340], grad_fn=<CatBackward>)
Batch 14/20:
reward seqs after 6 iterations: tensor([-0.0347], grad_fn=<CatBackward>)
Batch 15/20:
reward seqs after 4 iterations: tensor([-0.0337], grad_fn=<CatBackward>)
Batch 16/20:
reward seqs after 11 iterations: tensor([-0.0341], grad_fn=<CatBackward>)
Batch 17/20:
reward seqs after 3 iterations: tensor([-0.0339], grad_fn=<CatBackward>)
Batch 18/20:
reward seqs after 15 iterations: tensor([-0.0335], grad_fn=<CatBackward>)
Batch 19/20:
reward seqs after 10 iterations: tensor([-0.0340], grad_fn=<CatBackward>)
Loss: 0.033401720225811005
Optimal set of params:
mid_point: tensor([[0.4917, 0.1400, 0.5124]])
angle: tensor([0.6764], grad_fn=<SelectBackward>)
gripper_rate: tensor([0.0113], grad_fn=<ClampBackward1>)
Optimal init pose seq: tensor([[0.1797, 0.1400, 0.7628, 1.0000, 0.0000, 0.0000, 0.0000]],
       grad_fn=<SliceBackward>)
=============== Iteration 0 -> model_loss: tensor([0.0334]) ===============
torch.Size([1, 11, 14]) torch.Size([1, 40, 12])
Best init pose: tensor([[0.1797, 0.1400, 0.7628, 1.0000, 0.0000, 0.0000, 0.0000]])
Best model loss: tensor([0.0334])
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 0
Spinning at iteration 1
